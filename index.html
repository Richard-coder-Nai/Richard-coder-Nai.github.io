<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Ruiqian Nai</title>

    <meta name="author" content="Ruiqian Nai">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/ruiqian.png" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Ruiqian Nai
                  ‰Ω¥Áëû‰πæ
                </p>
                <p>
                  I am a third-year PhD student at the <a href="https://iiis.tsinghua.edu.cn/en/">Institute for Interdisciplinary Information Sciences, Tsinghua University</a>, advised by Prof. <a href="https://yang-gao.weebly.com/">Yang Gao</a>. Previously, I earned my B.S. degree from the <a href="https://www.au.tsinghua.edu.cn/en/">Department of Automation, Tsinghua University</a>.
                </p>
                <p>
                  My research focuses on advancing <strong>embodied intelligence</strong>‚Äîenabling robots to perceive, reason, and learn through cutting-edge machine learning technologies. I believe that the pursuit of embodied intelligence involves a cycle of understanding, reasoning, and reinforcement learning. Agents first acquire knowledge from data, make decisions based on this knowledge, and continuously improve through their own experiences.
                </p>
                <p style="text-align:center">
                  <a href="mailto:nrq22@mails.tsinghua.edu.cn">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=I0HLZAwAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/RuiqianNai">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Richard-coder-Nai">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:20%;max-width:20%">
                <a href="images/nairuiqian_casual.png"><img style="width:100%;max-width:100%;" alt="profile photo" src="images/nairuiqian_casual.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Selected Publications <span style="font-size: smaller;">(* indicates equal contribution)</span></h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <video width="100%" height="100%" muted="" autoplay="" loop="">
                  <source src="images/onetwovla.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </td>
              <td width="75%" valign="middle">
                <a href="https://one-two-vla.github.io/">
                  <papertitle>OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning</papertitle>
                </a>
                <br>
                <a href="https://fanqi-lin.github.io/">Fanqi Lin*</a>,
                <strong>Ruiqian Nai*</strong>,
                <a href="https://yingdong-hu.github.io/">Yingdong Hu*</a>,
                <a href="https://scholar.google.com/citations?user=FiP-TVUAAAAJ">Jiacheng You</a>,
                Junming Zhao,
                <a href="https://yang-gao.weebly.com/">Yang Gao</a>
                <br>
                Preprint, 2025
                <br>
                <a href="https://one-two-vla.github.io/">project page</a>
                /
                <a href="https://arxiv.org/abs/2505.11917">arXiv</a>
                /
                <a href="https://github.com/Fanqi-Lin/OneTwoVLA">code</a>
                /
                <a href="https://x.com/gao_young/status/1924793713798066637">X summary</a>
                <p></p>
                <p>
                  We introduce OneTwoVLA, a single unified vision-language-action model capable of both acting (System One)‚ö° and reasoning (System Two)ü§î. Importantly, it adaptively determines when to engage each mode.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <video width="100%" height="100%" muted="" autoplay="" loop="">
                  <source src="images/hub_homepage_compressed.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </td>
              <td width="75%" valign="middle">
                  <a href="https://hub-robot.github.io/">
                    <span class="papertitle">HuB: Learning Extreme Humanoid Balance</span>
                  </a>
                  <br>
                  <a href="https://tongzhangthu.github.io/">Tong Zhang*</a>,
                  <a href="https://github.com/ZhengBryan">Boyuan Zheng*</a>, 
                  <strong>Ruiqian Nai</strong>,
                  <a href="https://yingdong-hu.github.io/">Yingdong Hu</a>, 
                  <a href="https://wangyenjen.github.io/">Yen-Jen Wang</a>, 
                  <a href="https://jc043.github.io/">Geng Chen</a>, 
                  <a href="https://fanqi-lin.github.io/">Fanqi Lin</a>, 
                  <a href="https://github.com/Cata1ysttt">Jiongye Li</a>, 
                  <a href="https://chuye03.github.io/">Chuye Hong</a>, 
                  <a href="https://hybrid-robotics.berkeley.edu/koushil/">Koushil Sreenath</a>, 
                  <a href="https://yang-gao.weebly.com/">Yang Gao</a>
                  <br>
                  Preprint, 2025
                  <br>
                  <a href="https://hub-robot.github.io/">project page</a> / 
                  <a href="https://arxiv.org/abs/2505.07294">arXiv</a> /
                  <a href="https://x.com/TongZha22057330/status/1922092674644894041">X summary</a>
                  <p></p>
                  <p>
                    We propose <strong>HuB</strong> (<strong>Hu</strong>manoid <strong>B</strong>alance) ü§ñ, a framework that enables humanoids to perform challenging quasi-static balance tasks ‚öñÔ∏è, including extreme single-legged poses ü¶µ such as the Swallow Balance üïäÔ∏è and Bruce Lee's Kick ü¶∂ü•ã.
                  </p>
              </td>
            </tr>


            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <video width="100%" height="100%" muted="" autoplay="" loop="">
                  <source src="images/powersaving.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </td>
              <td width="75%" valign="middle">
                <a href="https://hard-to-sim.github.io/">
                  <papertitle>Fine-Tuning Hard-to-Simulate Objectives for Quadruped Locomotion: A Case Study on Total Power Saving</papertitle>
                </a>
                <br>
                <strong>Ruiqian Nai</strong>,
                <a href="https://scholar.google.com/citations?user=FiP-TVUAAAAJ">Jiacheng You</a>,
                <a href="https://github.com/xiaohu-art">Liu Cao</a>,
                <a href="https://morning-star-7.github.io/"> Hanchen Cui</a>,
                Shiyuan Zhang,
                <a href="http://hxu.rocks/">Huazhe Xu</a>,
                <a href="https://yang-gao.weebly.com/">Yang Gao</a>
                <br>
                <em>ICRA 2025</em> 
                <br>
                <a href="https://hard-to-sim.github.io/">project page</a>
                /
                <a href="https://arxiv.org/abs/2502.10956">arXiv</a>
                /
                <a href="https://x.com/RuiqianNai/status/1892397686403285458">X summary</a>
                <p></p>
                <p>
                  Better locomotion using <strong>real-world</strong> data. Our approach achieves a 24-28% net reduction in power consumption üîã.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/disentangle_downstream.png" width="200">
              </td>
              <td width="75%" valign="middle">
                <a href="https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=I0HLZAwAAAAJ&citation_for_view=I0HLZAwAAAAJ:d1gkVwhDpl0C">
                  <papertitle>Revisiting Disentanglement in Downstream Tasks: A Study on Its Necessity for Abstract Visual Reasoning</papertitle>
                </a>
                <br>
                <strong>Ruiqian Nai</strong>,
                <a href="https://zixinwen.com/">Zixin Wen</a>,
                Ji Li,
                <a href="https://mbzuai.ac.ae/study/faculty/yuanzhi-li/">Yuanzhi Li</a>,
                <a href="https://yang-gao.weebly.com/">Yang Gao</a>
                <br>
                <em>AAAI 2024</em>
                <br>
                <a href="https://arxiv.org/abs/2403.02233">arxiv</a>
                /
                <a href="https://github.com/Richard-coder-Nai/disentanglement-lib-necessity">code</a>
                <p></p>
                <p>
                  We show that <em>informativeness</em> üß† is a more crucial factor than <em>disentanglement</em> üåÄ in downstream tasks.
                </p>
              </td>
            </tr>


          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Modified from <a href="https://github.com/jonbarron/jonbarron.github.io">Jon Barron's website</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
